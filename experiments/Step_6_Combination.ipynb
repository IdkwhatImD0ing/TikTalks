{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import gc\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "import openvino_genai as ov_genai\n",
    "from transformers import pipeline\n",
    "import scipy\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "import dotenv\n",
    "import openai  # Ensure you have the 'openai' library installed\n",
    "from pathlib import Path\n",
    "from optimum.intel.openvino import OVStableDiffusionXLPipeline\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from moviepy.editor import (\n",
    "    ImageClip,\n",
    "    AudioFileClip,\n",
    "    concatenate_videoclips,\n",
    "    CompositeVideoClip,\n",
    "    TextClip,\n",
    "    CompositeAudioClip,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def generate_text(prompt, max_new_tokens, model_dir=\"./Llama-3.2-1B_instruct_openvino\"):\n",
    "    \"\"\"\n",
    "    Generates text using OpenVINO's LLMPipeline.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt (str): The input prompt for the language model.\n",
    "    - max_new_tokens (int): The maximum number of tokens to generate.\n",
    "    - model_dir (str): Directory where the model is stored.\n",
    "\n",
    "    Returns:\n",
    "    - str: Generated text.\n",
    "    \"\"\"\n",
    "    pipe = ov_genai.LLMPipeline(model_dir, \"CPU\")\n",
    "    text = pipe.generate(prompt, max_new_tokens=max_new_tokens)\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_music_description(script_text, generate_text_fn, temp_dir):\n",
    "    \"\"\"\n",
    "    Generates a description for the background music based on the script.\n",
    "\n",
    "    Parameters:\n",
    "    - script_text (str): The generated script text.\n",
    "    - generate_text_fn (function): Function to generate text.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - str: Description of the background music.\n",
    "    \"\"\"\n",
    "    music_prompt = \"\"\"\n",
    "    The above is a script for a TikTok video.\n",
    "    Please generate a short one-sentence description of the music that should be playing in the background of the video.\n",
    "    Include genre and mood.\n",
    "    Example:\n",
    "    \"A short upbeat EDM tune with a catchy melody\"\n",
    "\n",
    "    Music Description:\n",
    "    \"\"\"\n",
    "    combined_prompt = script_text + '\\n\\n' + music_prompt\n",
    "    music_description = generate_text_fn(combined_prompt, 100)\n",
    "    return music_description.strip()\n",
    "\n",
    "\n",
    "def generate_music(music_description, temp_dir):\n",
    "    \"\"\"\n",
    "    Generates background music using the MusicGen model.\n",
    "\n",
    "    Parameters:\n",
    "    - music_description (str): Description of the music.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the generated music file.\n",
    "    \"\"\"\n",
    "    synthesiser = pipeline(\"text-to-audio\", \"facebook/musicgen-small\")\n",
    "    music = synthesiser(\n",
    "        music_description,\n",
    "        forward_params={\"do_sample\": True}\n",
    "    )\n",
    "    # Ensure the output directory exists\n",
    "    output_music_path = os.path.join(temp_dir, \"background_music.wav\")\n",
    "    scipy.io.wavfile.write(output_music_path, rate=music[\"sampling_rate\"], data=music[\"audio\"])\n",
    "    del synthesiser, music\n",
    "    gc.collect()\n",
    "    return output_music_path\n",
    "\n",
    "\n",
    "# Original Working `synthesize_text_to_audio` Function\n",
    "def synthesize_text_to_audio(\n",
    "    text: str,\n",
    "    output_path: str = \"output/audio.wav\",\n",
    "    matcha_checkpoint: Path = None,\n",
    "    hifigan_checkpoint: Path = None,\n",
    "    n_timesteps: int = 10,\n",
    "    length_scale: float = 1.0,\n",
    "    temperature: float = 0.667,\n",
    "    device: torch.device = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Synthesizes speech from the input text and saves it as a WAV file.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to synthesize.\n",
    "    - output_path (str): Path to save the output WAV file.\n",
    "    - matcha_checkpoint (Path): Path to the Matcha-TTS checkpoint. Defaults to user data directory.\n",
    "    - hifigan_checkpoint (Path): Path to the HiFi-GAN checkpoint. Defaults to user data directory.\n",
    "    - n_timesteps (int): Number of ODE solver steps.\n",
    "    - length_scale (float): Changes to the speaking rate.\n",
    "    - temperature (float): Sampling temperature.\n",
    "    - device (torch.device): Device to run the models on. Defaults to CUDA if available.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Contains the synthesized waveform and related metadata.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import soundfile as sf\n",
    "    from matcha.hifigan.config import v1\n",
    "    from matcha.hifigan.denoiser import Denoiser\n",
    "    from matcha.hifigan.env import AttrDict\n",
    "    from matcha.hifigan.models import Generator as HiFiGAN\n",
    "    from matcha.models.matcha_tts import MatchaTTS\n",
    "    from matcha.text import sequence_to_text, text_to_sequence\n",
    "    from matcha.utils.utils import get_user_data_dir, intersperse\n",
    "\n",
    "    # Initialize device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set default checkpoint paths if not provided\n",
    "    if matcha_checkpoint is None:\n",
    "        matcha_checkpoint = get_user_data_dir() / \"matcha_ljspeech.ckpt\"\n",
    "    if hifigan_checkpoint is None:\n",
    "        hifigan_checkpoint = get_user_data_dir() / \"hifigan_T2_v1\"\n",
    "\n",
    "    # Initialize models only once\n",
    "    if not hasattr(synthesize_text_to_audio, \"model\"):\n",
    "        # Load Matcha-TTS model\n",
    "        print(\"Loading Matcha-TTS model...\")\n",
    "        synthesize_text_to_audio.model = MatchaTTS.load_from_checkpoint(\n",
    "            matcha_checkpoint, map_location=device\n",
    "        ).to(device)\n",
    "        synthesize_text_to_audio.model.eval()\n",
    "        print(\"Matcha-TTS model loaded.\")\n",
    "\n",
    "        # Load HiFi-GAN vocoder\n",
    "        print(\"Loading HiFi-GAN vocoder...\")\n",
    "        h = AttrDict(v1)\n",
    "        synthesize_text_to_audio.vocoder = HiFiGAN(h).to(device)\n",
    "        synthesize_text_to_audio.vocoder.load_state_dict(\n",
    "            torch.load(hifigan_checkpoint, map_location=device)[\"generator\"]\n",
    "        )\n",
    "        synthesize_text_to_audio.vocoder.eval()\n",
    "        synthesize_text_to_audio.vocoder.remove_weight_norm()\n",
    "        print(\"HiFi-GAN vocoder loaded.\")\n",
    "\n",
    "        # Initialize Denoiser\n",
    "        synthesize_text_to_audio.denoiser = Denoiser(synthesize_text_to_audio.vocoder, mode=\"zeros\")\n",
    "\n",
    "    model = synthesize_text_to_audio.model\n",
    "    vocoder = synthesize_text_to_audio.vocoder\n",
    "    denoiser = synthesize_text_to_audio.denoiser\n",
    "\n",
    "    # Define helper functions within the main function\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def process_text(text_input: str):\n",
    "        x = torch.tensor(\n",
    "            intersperse(text_to_sequence(text_input, ['english_cleaners2'])[0], 0),\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        ).unsqueeze(0)\n",
    "        x_lengths = torch.tensor([x.shape[-1]], dtype=torch.long, device=device)\n",
    "        x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "        return {\n",
    "            'x_orig': text_input,\n",
    "            'x': x,\n",
    "            'x_lengths': x_lengths,\n",
    "            'x_phones': x_phones\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def synthesise(text_processed):\n",
    "        start_time = dt.datetime.now()\n",
    "        output = model.synthesise(\n",
    "            text_processed['x'],\n",
    "            text_processed['x_lengths'],\n",
    "            n_timesteps=n_timesteps,\n",
    "            temperature=temperature,\n",
    "            spks=None,  # Modify if speaker embeddings are used\n",
    "            length_scale=length_scale\n",
    "        )\n",
    "        output.update({'start_t': start_time, **text_processed})\n",
    "        return output\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def to_waveform(mel_spec):\n",
    "        audio = vocoder(mel_spec).clamp(-1, 1)\n",
    "        audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "        return audio.numpy()\n",
    "\n",
    "    def save_audio(waveform, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        sf.write(path, waveform, 22050, subtype='PCM_24')\n",
    "        print(f\"Audio saved to {path}\")\n",
    "\n",
    "    # Process the input text\n",
    "    text_processed = process_text(text)\n",
    "\n",
    "    # Synthesize the mel spectrogram\n",
    "    output = synthesise(text_processed)\n",
    "\n",
    "    # Convert mel spectrogram to waveform\n",
    "    waveform = to_waveform(output['mel'])\n",
    "\n",
    "    # Save the waveform to the specified output path\n",
    "    save_audio(waveform, output_path)\n",
    "\n",
    "    # Optionally, return the waveform and other details\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path, temp_dir):\n",
    "    \"\"\"\n",
    "    Transcribes the audio file to generate subtitles with timestamps.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_path (str): Path to the audio file.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the transcription text file.\n",
    "    \"\"\"\n",
    "    transcription_fn = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=\"openai/whisper-tiny.en\",\n",
    "        chunk_length_s=30,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    audio_input, sr = sf.read(audio_path)\n",
    "    inputs = {\n",
    "        \"raw\": audio_input,\n",
    "        \"sampling_rate\": sr,\n",
    "    }\n",
    "\n",
    "    transcription_chunks = transcription_fn(\n",
    "        inputs,\n",
    "        batch_size=8,\n",
    "        return_timestamps=True\n",
    "    ).get(\"chunks\", [])\n",
    "\n",
    "    formatted_transcription = \"\"\n",
    "    for segment in transcription_chunks:\n",
    "        text = segment.get(\"text\", \"\").strip()\n",
    "        start, end = segment.get(\"timestamp\", (0.0, 0.0))\n",
    "        start_rounded = round(start, 2)\n",
    "        end_rounded = round(end, 2) if end else round(start + 5.0, 2)\n",
    "        formatted_transcription += f\"{text} | start: {start_rounded} | end: {end_rounded}\\n\"\n",
    "\n",
    "    transcription_path = os.path.join(temp_dir, \"transcription.txt\")\n",
    "    with open(transcription_path, 'w') as f:\n",
    "        f.write(formatted_transcription)\n",
    "\n",
    "    del transcription_fn, audio_input, sr, inputs, transcription_chunks, formatted_transcription\n",
    "    gc.collect()\n",
    "\n",
    "    return transcription_path\n",
    "\n",
    "\n",
    "def generate_image_descriptions(transcription, temp_dir):\n",
    "    \"\"\"\n",
    "    Generates image descriptions based on the transcription.\n",
    "\n",
    "    Parameters:\n",
    "    - transcription (str): Path to the transcription text file.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the JSON file containing image descriptions.\n",
    "    \"\"\"\n",
    "    script = \"\"\"\n",
    "You are given a transcript of a short video with timestamps.\n",
    "You are in charge of making a list of pictures that will be used to create a video.\n",
    "The video will be a slideshow of the pictures.\n",
    "The pictures should be relevant to the text.\n",
    "Make sure to include how long each picture should be displayed as well as the description of the picture.\n",
    "Make only 5 images.\n",
    "\n",
    "Example JSON output:\n",
    "{\"images\": [{\"description\": \"A picture of a cat\", \"start\": 1, \"end\": 3}, {\"description\": \"A picture of a dog\", \"start\": 3, \"end\": 5}]}  \n",
    "\"\"\"\n",
    "\n",
    "    with open(transcription, 'r') as f:\n",
    "        transcript_text = f.read()\n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": script},\n",
    "            {\"role\": \"user\", \"content\": transcript_text}\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        images = json.loads(response.choices[0].message.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"Failed to parse JSON from OpenAI response: {e}\")\n",
    "        images = []\n",
    "\n",
    "    images_path = os.path.join(temp_dir, \"images.json\")\n",
    "    with open(images_path, 'w') as f:\n",
    "        json.dump(images, f, indent=4)\n",
    "\n",
    "    del response, transcript_text, images\n",
    "    gc.collect()\n",
    "\n",
    "    return images_path\n",
    "\n",
    "\n",
    "def setup_sdxl_base_model(model_dir, device=\"CPU\", compress_weights=True):\n",
    "    \"\"\"\n",
    "    Sets up the Stable Diffusion XL Base model optimized with OpenVINO.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir (str): Directory to save the converted model.\n",
    "    - device (str): Inference device ('CPU', 'GPU.0', etc.).\n",
    "    - compress_weights (bool): Whether to apply 8-bit weight compression.\n",
    "\n",
    "    Returns:\n",
    "    - OVStableDiffusionXLPipeline: The optimized pipeline.\n",
    "    \"\"\"\n",
    "    quantization_config = {\"bits\": 8} if compress_weights else None\n",
    "\n",
    "    if not Path(model_dir).exists():\n",
    "        pipeline = OVStableDiffusionXLPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            compile=False,\n",
    "            device=device,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "        pipeline.half()\n",
    "        pipeline.save_pretrained(model_dir)\n",
    "        pipeline.compile()\n",
    "    else:\n",
    "        pipeline = OVStableDiffusionXLPipeline.from_pretrained(\n",
    "            model_dir,\n",
    "            device=device\n",
    "        )\n",
    "    gc.collect()\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def generate_images(images_json_path, base_pipeline, temp_dir):\n",
    "    \"\"\"\n",
    "    Generates images based on the provided descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - images_json_path (str): Path to the JSON file containing image descriptions.\n",
    "    - base_pipeline (OVStableDiffusionXLPipeline): The image generation pipeline.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of paths to the generated images.\n",
    "    \"\"\"\n",
    "    with open(images_json_path, 'r') as f:\n",
    "        images = json.load(f)\n",
    "\n",
    "    generated_images = []\n",
    "    print(images)\n",
    "    for idx, image_info in enumerate(images['images']):\n",
    "        description = image_info[\"description\"]\n",
    "        print(f\"Generating image {idx+1}: {description}\")\n",
    "        image = base_pipeline(\n",
    "            prompt=description,\n",
    "            num_inference_steps=15,\n",
    "            height=512,\n",
    "            width=512,\n",
    "            generator=np.random.RandomState(42)\n",
    "        ).images[0]\n",
    "        image_path = os.path.join(temp_dir, f\"image_{idx}.png\")\n",
    "        image.save(image_path)\n",
    "        generated_images.append(image_path)\n",
    "\n",
    "    del base_pipeline, images, image\n",
    "    gc.collect()\n",
    "\n",
    "    return generated_images\n",
    "\n",
    "\n",
    "def generate_subtitles(transcription_path, temp_dir):\n",
    "    \"\"\"\n",
    "    Formats the transcription into a JSON file suitable for subtitles.\n",
    "\n",
    "    Parameters:\n",
    "    - transcription_path (str): Path to the transcription text file.\n",
    "    - temp_dir (str): Path to the temporary directory.\n",
    "\n",
    "    Returns:\n",
    "    - str: Path to the JSON file containing subtitles.\n",
    "    \"\"\"\n",
    "    subtitles = []\n",
    "    with open(transcription_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        text = parts[0].strip()\n",
    "        start = float(parts[1].replace('start:', '').strip())\n",
    "        end = float(parts[2].replace('end:', '').strip())\n",
    "        subtitles.append({\"text\": text, \"start\": start, \"end\": end})\n",
    "\n",
    "    subtitles_path = os.path.join(temp_dir, \"subtitles.json\")\n",
    "    with open(subtitles_path, 'w') as f:\n",
    "        json.dump(subtitles, f, indent=4)\n",
    "\n",
    "    return subtitles_path\n",
    "\n",
    "\n",
    "def assemble_video(image_paths, voice_over_path, music_path, subtitles_path, output_path):\n",
    "    \"\"\"\n",
    "    Assembles the final video by stitching together images, adding audio, and overlaying subtitles.\n",
    "\n",
    "    Parameters:\n",
    "    - image_paths (list): List of paths to the generated images.\n",
    "    - voice_over_path (str): Path to the voice-over audio file.\n",
    "    - music_path (str): Path to the background music audio file.\n",
    "    - subtitles_path (str): Path to the subtitles JSON file.\n",
    "    - output_path (str): Path to save the final video.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Create Image Clips\n",
    "    clips = []\n",
    "    with open(subtitles_path, 'r') as f:\n",
    "        subtitles = json.load(f)\n",
    "\n",
    "    total_duration = max([s['end'] for s in subtitles]) if subtitles else 30\n",
    "    image_duration = total_duration / len(image_paths) if image_paths else total_duration\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        clip = ImageClip(image_path).set_duration(image_duration)\n",
    "        clips.append(clip)\n",
    "\n",
    "    if not clips:\n",
    "        logging.error(\"No images to assemble into video.\")\n",
    "        return\n",
    "\n",
    "    video = concatenate_videoclips(clips, method=\"compose\")\n",
    "\n",
    "    # Add Voice-Over Audio\n",
    "    voice_over = AudioFileClip(voice_over_path)\n",
    "    video = video.set_audio(voice_over)\n",
    "\n",
    "    # Add Background Music\n",
    "    music = AudioFileClip(music_path).volumex(0.1)  # Lower volume for background music\n",
    "    final_audio = CompositeAudioClip([voice_over, music])\n",
    "    video = video.set_audio(final_audio)\n",
    "\n",
    "    # Add Subtitles\n",
    "    for subtitle in subtitles:\n",
    "        txt_clip = TextClip(\n",
    "            subtitle[\"text\"],\n",
    "            fontsize=24,\n",
    "            color='white',\n",
    "            bg_color='black',\n",
    "            method='caption',\n",
    "            size=(video.w * 0.8, None)\n",
    "        )\n",
    "        txt_clip = txt_clip.set_start(subtitle[\"start\"]).set_end(subtitle[\"end\"]).set_position(('center', 'bottom'))\n",
    "        video = CompositeVideoClip([video, txt_clip])\n",
    "\n",
    "    # Write the final video\n",
    "    video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=24)\n",
    "\n",
    "    # Cleanup\n",
    "    del video, voice_over, music, final_audio, clips, txt_clip\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load environment variables from .env file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdotenv\u001b[49m\u001b[38;5;241m.\u001b[39mload_dotenv()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# with tempfile.TemporaryDirectory() as temp_dir:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m temp_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# with tempfile.TemporaryDirectory() as temp_dir:\n",
    "temp_dir = \"temp\"\n",
    "logging.info(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Step 1: Generate Text Script\n",
    "prompt = \"\"\"\\n\\n\n",
    "Your task is to create a 30 second engaging and educational TikTok script based on the following sentence:\n",
    "\n",
    "{input_sentence}\n",
    "\n",
    "Expand on this sentence to create an interesting and educational script that most people might not know about.\n",
    "The TikTok should incorporate an engaging story or example related to the sentence.\n",
    "Do not include any emojis or hashtags in the script.\n",
    "The script should be only spoken text, no extra text like [Cut] or [Music].\n",
    "The script should sound passionate, excited, and happy.\n",
    "\n",
    "Script:\n",
    "\"\"\"\n",
    "user_input = prompt.format(input_sentence=\"Spaceships are the future of human travel.\")\n",
    "script_text = generate_text(user_input, 1000)\n",
    "script_path = os.path.join(temp_dir, \"script.txt\")\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_text)\n",
    "logging.info(\"Generated text script.\")\n",
    "\n",
    "# Step 2: Generate Music Description\n",
    "music_description = generate_music_description(script_text, generate_text, temp_dir)\n",
    "logging.info(f\"Music Description: {music_description}\")\n",
    "\n",
    "# Step 3: Generate Background Music\n",
    "music_path = generate_music(music_description, temp_dir)\n",
    "logging.info(f\"Background music saved at {music_path}\")\n",
    "\n",
    "# Step 4: Generate Voice-Over Audio\n",
    "voice_over_path = synthesize_text_to_audio(script_text, os.path.join(temp_dir, \"voice_over.wav\"))\n",
    "logging.info(f\"Voice-over audio saved at {voice_over_path}\")\n",
    "\n",
    "# Step 5: Transcribe Audio to Generate Subtitles\n",
    "transcription_path = transcribe_audio(voice_over_path, temp_dir)\n",
    "logging.info(f\"Transcription saved at {transcription_path}\")\n",
    "\n",
    "# Step 6: Generate Image Descriptions\n",
    "images_json_path = generate_image_descriptions(transcription_path, temp_dir)\n",
    "logging.info(f\"Image descriptions saved at {images_json_path}\")\n",
    "\n",
    "# Step 7: Generate Images\n",
    "model_dir = os.path.join(temp_dir, \"openvino-sd-xl-base-1.0\")\n",
    "base_pipeline = setup_sdxl_base_model(model_dir=model_dir, device=\"CPU\")\n",
    "images_paths = generate_images(images_json_path, base_pipeline, temp_dir)\n",
    "logging.info(f\"Generated images: {images_paths}\")\n",
    "\n",
    "# Step 8: Generate Subtitles File\n",
    "subtitles_path = generate_subtitles(transcription_path, temp_dir)\n",
    "logging.info(f\"Subtitles saved at {subtitles_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy.config import change_settings\n",
    "\n",
    "change_settings({\"IMAGEMAGICK_BINARY\": \"/usr/bin/convert\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video temp/final_video.mp4.\n",
      "MoviePy - Writing audio in final_videoTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video temp/final_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 13:14:40,081 - INFO - Final video saved at temp/final_video.mp4\n",
      "2024-09-27 13:14:40,086 - INFO - Video moved to /mnt/c/Users/intelaipc/Documents/TikTalks/experiments/final_video.mp4\n",
      "2024-09-27 13:14:40,087 - INFO - All done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready temp/final_video.mp4\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Assemble the Video\n",
    "temp_dir = \"temp\"\n",
    "voice_over_path = os.path.join(temp_dir, \"voice_over.wav\")\n",
    "music_path = os.path.join(temp_dir, \"background_music.wav\")\n",
    "subtitles_path = os.path.join(temp_dir, \"subtitles.json\")\n",
    "images_paths = [os.path.join(temp_dir, f\"image_{i}.png\") for i in range(0, 5)]  # Assuming 5 images\n",
    "output_video_path = os.path.join(temp_dir, \"final_video.mp4\")\n",
    "assemble_video(images_paths, voice_over_path, music_path, subtitles_path, output_video_path)\n",
    "logging.info(f\"Final video saved at {output_video_path}\")\n",
    "\n",
    "# Move the final video to the current directory\n",
    "final_output = os.path.join(os.getcwd(), \"final_video.mp4\")\n",
    "shutil.move(output_video_path, final_output)\n",
    "logging.info(f\"Video moved to {final_output}\")\n",
    "\n",
    "logging.info(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
