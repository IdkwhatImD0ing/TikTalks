{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Matcha-TTS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intelaipc/intel/oneapi/intelpython/envs/tiktalk/lib/python3.10/site-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "/home/intelaipc/intel/oneapi/intelpython/envs/tiktalk/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcha-TTS model loaded.\n",
      "Loading HiFi-GAN vocoder...\n",
      "Removing weight norm...\n",
      "HiFi-GAN vocoder loaded.\n",
      "Audio saved to output/audio.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import necessary modules from Matcha and HiFi-GAN\n",
    "from matcha.hifigan.config import v1\n",
    "from matcha.hifigan.denoiser import Denoiser\n",
    "from matcha.hifigan.env import AttrDict\n",
    "from matcha.hifigan.models import Generator as HiFiGAN\n",
    "from matcha.models.matcha_tts import MatchaTTS\n",
    "from matcha.text import sequence_to_text, text_to_sequence\n",
    "from matcha.utils.model import denormalize\n",
    "from matcha.utils.utils import get_user_data_dir, intersperse\n",
    "\n",
    "def synthesize_text_to_audio(\n",
    "    text: str,\n",
    "    output_path: str = \"output/audio.wav\",\n",
    "    matcha_checkpoint: Path = None,\n",
    "    hifigan_checkpoint: Path = None,\n",
    "    n_timesteps: int = 10,\n",
    "    length_scale: float = 1.0,\n",
    "    temperature: float = 0.667,\n",
    "    device: torch.device = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Synthesizes speech from the input text and saves it as a WAV file.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to synthesize.\n",
    "    - output_path (str): Path to save the output WAV file.\n",
    "    - matcha_checkpoint (Path): Path to the Matcha-TTS checkpoint. Defaults to user data directory.\n",
    "    - hifigan_checkpoint (Path): Path to the HiFi-GAN checkpoint. Defaults to user data directory.\n",
    "    - n_timesteps (int): Number of ODE solver steps.\n",
    "    - length_scale (float): Changes to the speaking rate.\n",
    "    - temperature (float): Sampling temperature.\n",
    "    - device (torch.device): Device to run the models on. Defaults to CUDA if available.\n",
    "    \"\"\"\n",
    "    # Initialize device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set default checkpoint paths if not provided\n",
    "    if matcha_checkpoint is None:\n",
    "        matcha_checkpoint = get_user_data_dir() / \"matcha_ljspeech.ckpt\"\n",
    "    if hifigan_checkpoint is None:\n",
    "        hifigan_checkpoint = get_user_data_dir() / \"hifigan_T2_v1\"\n",
    "\n",
    "    # Initialize models only once\n",
    "    if not hasattr(synthesize_text_to_audio, \"model\"):\n",
    "        # Load Matcha-TTS model\n",
    "        print(\"Loading Matcha-TTS model...\")\n",
    "        synthesize_text_to_audio.model = MatchaTTS.load_from_checkpoint(\n",
    "            matcha_checkpoint, map_location=device\n",
    "        ).to(device)\n",
    "        synthesize_text_to_audio.model.eval()\n",
    "        synthesize_text_to_audio.model = synthesize_text_to_audio.model.to(device)\n",
    "        print(\"Matcha-TTS model loaded.\")\n",
    "\n",
    "        # Load HiFi-GAN vocoder\n",
    "        print(\"Loading HiFi-GAN vocoder...\")\n",
    "        h = AttrDict(v1)\n",
    "        synthesize_text_to_audio.vocoder = HiFiGAN(h).to(device)\n",
    "        synthesize_text_to_audio.vocoder.load_state_dict(\n",
    "            torch.load(hifigan_checkpoint, map_location=device)[\"generator\"]\n",
    "        )\n",
    "        synthesize_text_to_audio.vocoder.eval()\n",
    "        synthesize_text_to_audio.vocoder.remove_weight_norm()\n",
    "        print(\"HiFi-GAN vocoder loaded.\")\n",
    "\n",
    "        # Initialize Denoiser\n",
    "        synthesize_text_to_audio.denoiser = Denoiser(synthesize_text_to_audio.vocoder, mode=\"zeros\")\n",
    "\n",
    "    model = synthesize_text_to_audio.model\n",
    "    vocoder = synthesize_text_to_audio.vocoder\n",
    "    denoiser = synthesize_text_to_audio.denoiser\n",
    "\n",
    "    # Define helper functions within the main function\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def process_text(text_input: str):\n",
    "        x = torch.tensor(\n",
    "            intersperse(text_to_sequence(text_input, ['english_cleaners2'])[0], 0),\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        ).unsqueeze(0)\n",
    "        x_lengths = torch.tensor([x.shape[-1]], dtype=torch.long, device=device)\n",
    "        x_phones = sequence_to_text(x.squeeze(0).tolist())\n",
    "        return {\n",
    "            'x_orig': text_input,\n",
    "            'x': x,\n",
    "            'x_lengths': x_lengths,\n",
    "            'x_phones': x_phones\n",
    "        }\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def synthesise(text_processed):\n",
    "        start_time = dt.datetime.now()\n",
    "        output = model.synthesise(\n",
    "            text_processed['x'],\n",
    "            text_processed['x_lengths'],\n",
    "            n_timesteps=n_timesteps,\n",
    "            temperature=temperature,\n",
    "            spks=None,  # Modify if speaker embeddings are used\n",
    "            length_scale=length_scale\n",
    "        )\n",
    "        output.update({'start_t': start_time, **text_processed})\n",
    "        return output\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def to_waveform(mel_spec):\n",
    "        audio = vocoder(mel_spec).clamp(-1, 1)\n",
    "        audio = denoiser(audio.squeeze(0), strength=0.00025).cpu().squeeze()\n",
    "        return audio.numpy()\n",
    "\n",
    "    def save_audio(waveform, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        sf.write(path, waveform, 22050, subtype='PCM_24')\n",
    "        print(f\"Audio saved to {path}\")\n",
    "\n",
    "    # Process the input text\n",
    "    text_processed = process_text(text)\n",
    "    \n",
    "    # Synthesize the mel spectrogram\n",
    "    output = synthesise(text_processed)\n",
    "    \n",
    "    # Convert mel spectrogram to waveform\n",
    "    waveform = to_waveform(output['mel'])\n",
    "    \n",
    "    # Save the waveform to the specified output path\n",
    "    save_audio(waveform, output_path)\n",
    "    \n",
    "    # Optionally, return the waveform and other details\n",
    "    return {\n",
    "        'text': output['x_orig'],\n",
    "        'phonetic': output['x_phones'],\n",
    "        'waveform': waveform,\n",
    "        'rtf': None  # Real-Time Factor can be computed if needed\n",
    "    }\n",
    "\n",
    "\n",
    "result = synthesize_text_to_audio(\"Imagine a world where humanity has finally reached the stars, and we\\'re not just talking about any old spacecraft, but the most advanced, cutting-edge, and sustainable vessels that are changing the game. Spaceships are not just a luxury, they\\'re a necessity. They\\'re the key to unlocking new frontiers, new discoveries, and new possibilities for humanity. But did you know that the first spaceship was actually a hot air balloon? Yes, you heard that right! In 1783, French inventor Montgolfier created the first successful hot air balloon, which carried a group of 20 people to the skies. It was a groundbreaking achievement that paved the way for the development of modern space travel. Fast forward to today, and we have reusable rockets, advanced propulsion systems, and even private space companies like SpaceX and Blue Origin pushing the boundaries of what\\'s possible. But what\\'s even more exciting is that we\\'re not just talking about the technology, we\\'re talking about the people, the communities, and the cultures that are being shaped by space exploration. From the astronauts who are pushing the limits of human endurance to the scientists who are unlocking the secrets of the universe, space travel is not just a dream, it\\'s a reality that\\'s changing our world. So let\\'s get ready to blast off into the unknown, and explore the infinite possibilities that await us in the cosmos!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
