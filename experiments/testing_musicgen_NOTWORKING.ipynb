{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "from IPython.display import Audio\n",
    "import openvino as ov\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.jit import TracerWarning\n",
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    ")\n",
    "\n",
    "# Ignore tracing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intelaipc/intel/oneapi/intelpython/envs/tiktalk/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/intelaipc/intel/oneapi/intelpython/envs/tiktalk/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from packaging.version import parse\n",
    "\n",
    "\n",
    "if sys.version_info < (3, 8):\n",
    "    import importlib_metadata\n",
    "else:\n",
    "    import importlib.metadata as importlib_metadata\n",
    "loading_kwargs = {}\n",
    "\n",
    "if parse(importlib_metadata.version(\"transformers\")) >= parse(\"4.40.0\"):\n",
    "    loading_kwargs[\"attn_implementation\"] = \"eager\"\n",
    "\n",
    "\n",
    "# Load the pipeline\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\", torchscript=True, return_dict=False, **loading_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate is 32000 Hz\n"
     ]
    }
   ],
   "source": [
    "sample_length = 29  # seconds\n",
    "\n",
    "n_tokens = sample_length * model.config.audio_encoder.frame_rate + 3\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"80s pop track with bassy drums and synth\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=n_tokens)\n",
    "\n",
    "Audio(audio_values[0].cpu().numpy(), rate=sampling_rate)\n",
    "print(\"Sampling rate is\", sampling_rate, \"Hz\")\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "t5_ir_path = models_dir / \"t5.xml\"\n",
    "musicgen_0_ir_path = models_dir / \"mg_0.xml\"\n",
    "musicgen_ir_path = models_dir / \"mg.xml\"\n",
    "audio_decoder_ir_path = models_dir / \"encodec.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intelaipc/intel/oneapi/intelpython/envs/tiktalk/lib/python3.10/site-packages/transformers/modeling_utils.py:4713: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if not t5_ir_path.exists():\n",
    "    t5_ov = ov.convert_model(model.text_encoder, example_input={\"input_ids\": inputs[\"input_ids\"]})\n",
    "\n",
    "    ov.save_model(t5_ov, t5_ir_path)\n",
    "    del t5_ov\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model config `torchscript` to True, so the model returns a tuple as output\n",
    "model.decoder.config.torchscript = True\n",
    "\n",
    "if not musicgen_0_ir_path.exists():\n",
    "    decoder_input = {\n",
    "        \"input_ids\": torch.ones(8, 1, dtype=torch.int64),\n",
    "        \"encoder_hidden_states\": torch.ones(2, 12, 1024, dtype=torch.float32),\n",
    "        \"encoder_attention_mask\": torch.ones(2, 12, dtype=torch.int64),\n",
    "    }\n",
    "    mg_ov_0_step = ov.convert_model(model.decoder, example_input=decoder_input)\n",
    "\n",
    "    ov.save_model(mg_ov_0_step, musicgen_0_ir_path)\n",
    "    del mg_ov_0_step\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional argument to the example_input dict\n",
    "if not musicgen_ir_path.exists():\n",
    "    # Add `past_key_values` to the converted model signature\n",
    "    decoder_input[\"past_key_values\"] = tuple(\n",
    "        [\n",
    "            (\n",
    "                torch.ones(2, 16, 1, 64, dtype=torch.float32),\n",
    "                torch.ones(2, 16, 1, 64, dtype=torch.float32),\n",
    "                torch.ones(2, 16, 12, 64, dtype=torch.float32),\n",
    "                torch.ones(2, 16, 12, 64, dtype=torch.float32),\n",
    "            )\n",
    "        ]\n",
    "        * 24\n",
    "    )\n",
    "\n",
    "    mg_ov = ov.convert_model(model.decoder, example_input=decoder_input)\n",
    "    for input in mg_ov.inputs[3:]:\n",
    "        input.get_node().set_partial_shape(ov.PartialShape([-1, 16, -1, 64]))\n",
    "        input.get_node().set_element_type(ov.Type.f32)\n",
    "\n",
    "    mg_ov.validate_nodes_and_infer_types()\n",
    "\n",
    "    ov.save_model(mg_ov, musicgen_ir_path)\n",
    "    del mg_ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not audio_decoder_ir_path.exists():\n",
    "\n",
    "    class AudioDecoder(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, output_ids):\n",
    "            return self.model.decode(output_ids, [None])\n",
    "\n",
    "    audio_decoder_input = {\"output_ids\": torch.ones((1, 1, 4, n_tokens - 3), dtype=torch.int64)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        audio_decoder_ov = ov.convert_model(AudioDecoder(model.audio_encoder), example_input=audio_decoder_input)\n",
    "    ov.save_model(audio_decoder_ov, audio_decoder_ir_path)\n",
    "    del audio_decoder_ov\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[ERROR] 07:20:28.920 [NPUBackends] Cannot find backend for inference. Make sure the device is available.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9e2d9399774c2b9be9cd347211572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"./models\")\n",
    "t5_ir_path = models_dir / \"t5.xml\"\n",
    "musicgen_0_ir_path = models_dir / \"mg_0.xml\"\n",
    "musicgen_ir_path = models_dir / \"mg.xml\"\n",
    "audio_decoder_ir_path = models_dir / \"encodec.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, encoder_ir, config):\n",
    "        super().__init__()\n",
    "        self.encoder = core.compile_model(encoder_ir, device.value)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        last_hidden_state = self.encoder(input_ids)[self.encoder.outputs[0]]\n",
    "        last_hidden_state = torch.tensor(last_hidden_state)\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "\n",
    "\n",
    "class MusicGenWrapper(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        music_gen_lm_0_ir,\n",
    "        music_gen_lm_ir,\n",
    "        config,\n",
    "        num_codebooks,\n",
    "        build_delay_pattern_mask,\n",
    "        apply_delay_pattern_mask,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.music_gen_lm_0 = core.compile_model(music_gen_lm_0_ir, device.value)\n",
    "        self.music_gen_lm = core.compile_model(music_gen_lm_ir, device.value)\n",
    "        self.config = config\n",
    "        self.num_codebooks = num_codebooks\n",
    "        self.build_delay_pattern_mask = build_delay_pattern_mask\n",
    "        self.apply_delay_pattern_mask = apply_delay_pattern_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        encoder_hidden_states: torch.FloatTensor = None,\n",
    "        encoder_attention_mask: torch.LongTensor = None,\n",
    "        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if past_key_values is None:\n",
    "            model = self.music_gen_lm_0\n",
    "            arguments = (input_ids, encoder_hidden_states, encoder_attention_mask)\n",
    "        else:\n",
    "            model = self.music_gen_lm\n",
    "            arguments = (\n",
    "                input_ids,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                *past_key_values,\n",
    "            )\n",
    "\n",
    "        output = model(arguments)\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            logits=torch.tensor(output[model.outputs[0]]),\n",
    "            past_key_values=tuple([output[model.outputs[i]] for i in range(1, 97)]),\n",
    "        )\n",
    "\n",
    "\n",
    "class AudioDecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, decoder_ir, config):\n",
    "        super().__init__()\n",
    "        self.decoder = core.compile_model(decoder_ir, device.value)\n",
    "        self.config = config\n",
    "        self.output_type = namedtuple(\"AudioDecoderOutput\", [\"audio_values\"])\n",
    "\n",
    "    def decode(self, output_ids, audio_scales):\n",
    "        output = self.decoder(output_ids)[self.decoder.outputs[0]]\n",
    "        return self.output_type(audio_values=torch.tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[ERROR] 07:20:33.216 [NPUBackends] Cannot find backend for inference. Make sure the device is available.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\", torchscript=True, return_dict=False)\n",
    "text_encode_ov = TextEncoderWrapper(t5_ir_path, model.text_encoder.config)\n",
    "musicgen_decoder_ov = MusicGenWrapper(\n",
    "    musicgen_0_ir_path,\n",
    "    musicgen_ir_path,\n",
    "    model.decoder.config,\n",
    "    model.decoder.num_codebooks,\n",
    "    model.decoder.build_delay_pattern_mask,\n",
    "    model.decoder.apply_delay_pattern_mask,\n",
    ")\n",
    "audio_encoder_ov = AudioDecoderWrapper(audio_decoder_ir_path, model.audio_encoder.config)\n",
    "\n",
    "del model.text_encoder\n",
    "del model.decoder\n",
    "del model.audio_encoder\n",
    "gc.collect()\n",
    "\n",
    "model.text_encoder = text_encode_ov\n",
    "model.decoder = musicgen_decoder_ov\n",
    "model.audio_encoder = audio_encoder_ov\n",
    "\n",
    "\n",
    "def prepare_inputs_for_generation(\n",
    "    self,\n",
    "    decoder_input_ids,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    head_mask=None,\n",
    "    decoder_attention_mask=None,\n",
    "    decoder_head_mask=None,\n",
    "    cross_attn_head_mask=None,\n",
    "    use_cache=None,\n",
    "    encoder_outputs=None,\n",
    "    decoder_delay_pattern_mask=None,\n",
    "    guidance_scale=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if decoder_delay_pattern_mask is None:\n",
    "        (\n",
    "            decoder_input_ids,\n",
    "            decoder_delay_pattern_mask,\n",
    "        ) = self.decoder.build_delay_pattern_mask(\n",
    "            decoder_input_ids,\n",
    "            self.generation_config.pad_token_id,\n",
    "            max_length=self.generation_config.max_length,\n",
    "        )\n",
    "\n",
    "    # apply the delay pattern mask\n",
    "    decoder_input_ids = self.decoder.apply_delay_pattern_mask(decoder_input_ids, decoder_delay_pattern_mask)\n",
    "\n",
    "    if guidance_scale is not None and guidance_scale > 1:\n",
    "        # for classifier free guidance we need to replicate the decoder args across the batch dim (we'll split these\n",
    "        # before sampling)\n",
    "        decoder_input_ids = decoder_input_ids.repeat((2, 1))\n",
    "        if decoder_attention_mask is not None:\n",
    "            decoder_attention_mask = decoder_attention_mask.repeat((2, 1))\n",
    "\n",
    "    if past_key_values is not None:\n",
    "        # cut decoder_input_ids if past is used\n",
    "        decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "        \"encoder_outputs\": encoder_outputs,\n",
    "        \"past_key_values\": past_key_values,\n",
    "        \"decoder_input_ids\": decoder_input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"decoder_attention_mask\": decoder_attention_mask,\n",
    "        \"head_mask\": head_mask,\n",
    "        \"decoder_head_mask\": decoder_head_mask,\n",
    "        \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "        \"use_cache\": use_cache,\n",
    "    }\n",
    "\n",
    "\n",
    "model.prepare_inputs_for_generation = partial(prepare_inputs_for_generation, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = 30  # seconds\n",
    "\n",
    "n_tokens = sample_length * model.config.audio_encoder.frame_rate + 3\n",
    "sampling_rate = model.config.audio_encoder.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"A high-energy, futuristic electronic dance track with a driving beat and synthesized leads, perfect for a space-themed video that\\'s out of this world!\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
    "\n",
    "scipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
